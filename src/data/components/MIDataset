import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from torchvision.datasets import DatasetFolder
from torchvision.transforms import ToTensor

class MIDataset(Dataset):
    def __init__(self, csv_file, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        # RoBERTa tokens
        
        # Openface features
        self.data_frame = pd.read_csv(csv_file)
        self.transform = transform

    def __len__(self):
        return len(self.data_frame)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        # Assume the last column is the label
        label = self.data_frame.iloc[idx, -1]
        # Assume all other columns are features
        features = self.data_frame.iloc[idx, :-1].values.astype('float')

        sample = {'features': features, 'label': label}

        if self.transform:
            sample = self.transform(sample)

        return sample


# composite dataset example
# Define the root directory where the CSV files are located
root = 'path_to_root_directory'

# Define the transform to convert the data to tensors
transform = ToTensor()

# Create the composite dataset using DatasetFolder
dataset = DatasetFolder(root=root, loader='csv', extensions='.csv', transform=transform)

# Create a DataLoader
dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)

# Iterate through the DataLoader
for batch in dataloader:
    print(batch['data'], batch['label'])

# composite dataset example
class CompositeFeatureDataset(Dataset):
    def __init__(self, root, transform=None):
        self.datasets = [
            FeatureDataset1(root, transform=transform),
            FeatureDataset2(root, transform=transform),
            FeatureDataset3(root, transform=transform)
        ]
        self.transform = transform
    
    def __getitem__(self, index):
        # Gather features from each dataset
        features = [dataset[index][0] for dataset in self.datasets]
        labels = [dataset[index][1] for dataset in self.datasets]
        
        # Concatenate features from all datasets
        combined_features = np.concatenate(features, axis=-1)
        
        # Assuming the labels are the same across datasets, return the first label
        label = labels[0]
        
        if self.transform is not None:
            combined_features = self.transform(combined_features)
        
        return combined_features, label
    
    def __len__(self):
        # Assuming all datasets are of equal length
        return len(self.datasets[0])


if __name__ == "__main__":
    # Usage
    csv_file = 'path_to_your_data.csv'
    dataset = MIDataset(csv_file=csv_file)

    # Create a DataLoader
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)

    # Iterate through the DataLoader
    for batch in dataloader:
        print(batch['features'], batch['label'])